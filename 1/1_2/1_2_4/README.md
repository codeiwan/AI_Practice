[Home](./../../../README.md) | [인공 지능 딥러닝의 이해](./../../README.md) | [딥러닝 맛보기](./../README.md) | 머신러닝 방식의 신경망 함수 생성과 사용

## 머신러닝 방식의 신경망 함수 생성과 사용
이번엔 ML 방식으로 인공 신경망 함수를 학습시키고 학습된 함수를 사용하는 과정을 살펴보자.  
다음과 같은 모양의 인공 신경망을 구성하고 학습시켜 보자.  
![Image](https://github.com/user-attachments/assets/8cc8242b-769b-44c3-bc27-cbee30b5cc99)

먼저 다음과 같은 숫자들의 집합 $X$, $Y$의 관계를 살펴보자.  
||1|2|3|4|5|6|
|---|---|---|---|---|---|---|
|X:|-1|0|1|2|3|4|
|Y:|-2|1|4|7|10|13|

$X$, $Y$ 숫자들을 보면, $X$ 값은 왼쪽에서 오른쪽으로 1씩, $Y$ 값은 3씩 증가하는 것을 눈치 챘을 수 있다.  
이 경우 아마도 $Y$는 $3 \times X$ 더하기 또는 빼기 얼마와 같다고 생각할 것이다.  
그러고 나서, 아마도 $X$가 0일 때, $Y$가 1인 것을 보았을 것이다.  
그리고 마침내 관계식 $Y = 3 \times X + 1$에 도달했을 것이다.

이와 같이 유추해 낸 방식은 인공 신경망이 데이터($X$, $Y$ 값)를 이용하여 학습(또는 훈련)을 통해 데이터($X$, $Y$)간의 관계를 발견하는 방식과 같다.

이제 이 식을 인공 신경망이 유추해 내는 과정을 살펴보자.  
그러기 위해 먼저 필요한 것은 무엇일까?  
바로 데이터이다.  
다음으로 필요한 것은 학습되지 않은 인공 신경망이다.  
인공 신경망에 $X$ 집합과 $Y$ 집합을 주면, 인공 신경망은 학습을 통해 $X$와 $Y$간의 관계를 알아낼 수 있어야 한다.  
인공 신경망 함수는 일반적으로 인공 신경망 모델이라고 한다.  
모델은 우리말로 모형을 의미하며, 함수 모형으로 이해할 수 있다.  
즉, 학습을 통해 만들어진 인공 신경망 함수는 $Y = 3 \times X + 1$ 함수의 모형 함수이다.  
인공 신경망 함수는 $Y = 3 \times X + 1$ 함수를 흉내 내는 함수라고 할 수 있다.  
그래서 인공 신경망 함수를 근사 함수라고도 한다.

<br/><br/>
다음과 같은 예제를 작성한다.
```python
import tensorflow as tf
import numpy as np

xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0])
ys = np.array([-2.0, 1.0, 4.0, 7.0, 10., 13.])

model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(1,)),
    tf.keras.layers.Dense(1)
])

model.compile(optimizer='sgd', loss='mean_squared_error')
model.fit(xs, ys, epochs=5)

p = model.predict(np.array([10.0]))

print(f'p: {p}')
```
```
Epoch 1/5
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 191ms/step - loss: 23.7037
Epoch 2/5
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 61ms/step - loss: 18.6641
Epoch 3/5
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 62ms/step - loss: 14.6989
Epoch 4/5
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 61ms/step - loss: 11.5790
Epoch 5/5
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 60ms/step - loss: 9.1241
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 53ms/step
p: [[20.757849]]
```
model.fit 함수 내에서 5회 학습이 수행된다.  
loss는 오차를 나타낸다. 학습이 진행될수록 오차가 줄어드는 것을 확인한다.  
오차에 대해서는 뒤에서 자세히 살펴보겠다.  
p는 model.predict 함수를 수행한 결과 값이다.  
입력 값 10.0에 대하여 20.757849를 출력한다.  
우리는 31에 가까운 값이 출력되기를 기대하고 있다.

<br/><br/>
예제를 다음과 같이 수정한다.
```python
model.fit(xs, ys, epochs=50)
```
학습을 50회 수행시켜 본다.

```
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 60ms/step - loss: 0.0037
Epoch 46/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 61ms/step - loss: 0.0033
Epoch 47/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 44ms/step - loss: 0.0030
Epoch 48/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 59ms/step - loss: 0.0027
Epoch 49/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 59ms/step - loss: 0.0024
Epoch 50/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 41ms/step - loss: 0.0023
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 72ms/step
p: [[30.793182]]
```
입력 값 10.0에 대하여 30.793182를 출력한다.  
31에 충분히 가까운 값이 출력회는 것을 볼 수 있다.  
훈련이 진행되면서 손실은 더 작아진다.

<br/><br/>
예제를 다음과 같이 수정한다.
```python
model.fit(xs, ys, epochs=500)
```
학습을 500회 수행시켜 본다.

```
Epoch 496/500
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 52ms/step - loss: 1.9556e-07
Epoch 497/500
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 53ms/step - loss: 1.9151e-07
Epoch 498/500
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 71ms/step - loss: 1.8766e-07
Epoch 499/500
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 129ms/step - loss: 1.8384e-07
Epoch 500/500
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 59ms/step - loss: 1.8003e-07
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 56ms/step
p: [[31.001238]]
```
입력 값 10.0에 대하여 31.001238을 출력한다.  
31에 더 가까워진 값이 출력되는 것을 볼 수 있다.  
학습을 500회 수행했을 때, 31에 충분히 가까운 결과 값이 출력되는 것을 볼 수 있다.  
여기서 학습시킨 인공 신경망 함수는 $Y = 3 \times X + 1$ 함수를 흉내 내는 근사함수이다.
